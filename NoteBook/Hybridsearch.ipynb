{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7796e974-8562-4d81-bef0-cd63f19228d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /usr/local/python/3.12.1/lib/python3.12/site-packages (4.1.0)\n",
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from sentence-transformers) (4.52.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (2.6.0+cpu)\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from sentence-transformers) (0.32.4)\n",
      "Requirement already satisfied: Pillow in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/python/3.12.1/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.1.31)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ede5f54e-0bee-496b-b9a2-c2ac9051c1ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keybert\n",
      "  Downloading keybert-0.9.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/codespace/.local/lib/python3.12/site-packages (from keybert) (2.2.4)\n",
      "Requirement already satisfied: rich>=10.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from keybert) (14.0.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in /home/codespace/.local/lib/python3.12/site-packages (from keybert) (1.6.1)\n",
      "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from keybert) (4.1.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich>=10.4.0->keybert) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn>=0.22.2->keybert) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn>=0.22.2->keybert) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn>=0.22.2->keybert) (3.6.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from sentence-transformers>=0.3.8->keybert) (4.52.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from sentence-transformers>=0.3.8->keybert) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers>=0.3.8->keybert) (2.6.0+cpu)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from sentence-transformers>=0.3.8->keybert) (0.32.4)\n",
      "Requirement already satisfied: Pillow in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers>=0.3.8->keybert) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers>=0.3.8->keybert) (4.12.2)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (3.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2024.6.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (1.1.3)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/python/3.12.1/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2025.1.31)\n",
      "Downloading keybert-0.9.0-py3-none-any.whl (41 kB)\n",
      "Installing collected packages: keybert\n",
      "Successfully installed keybert-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md\n",
    "!pip install keybert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4416b89-7596-4f78-a831-39e6f232df77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: elasticsearch<9 in /usr/local/python/3.12.1/lib/python3.12/site-packages (8.18.1)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.15.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from elasticsearch<9) (8.17.1)\n",
      "Requirement already satisfied: python-dateutil in /home/codespace/.local/lib/python3.12/site-packages (from elasticsearch<9) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions in /home/codespace/.local/lib/python3.12/site-packages (from elasticsearch<9) (4.12.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in /home/codespace/.local/lib/python3.12/site-packages (from elastic-transport<9,>=8.15.1->elasticsearch<9) (2.3.0)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from elastic-transport<9,>=8.15.1->elasticsearch<9) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil->elasticsearch<9) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"elasticsearch<9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbfb9fb3-560b-4ae8-99e5-c895b6b6db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0845664-56ae-4741-a0c7-e901b75e80e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\n",
    "    hosts=[\"http://localhost:9200\"],\n",
    "    # Nếu không cần auth thì không cần user/pass\n",
    ")\n",
    "# Test kết nối\n",
    "#assert es.ping(), \"Elasticsearch không kết nối được!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7966753-dbdd-41c6-b83a-901365080ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\" : \"941bc3af368d\",\n",
      "  \"cluster_name\" : \"docker-cluster\",\n",
      "  \"cluster_uuid\" : \"6KQrMvN8QuyudDcEoPEIBA\",\n",
      "  \"version\" : {\n",
      "    \"number\" : \"8.13.0\",\n",
      "    \"build_flavor\" : \"default\",\n",
      "    \"build_type\" : \"docker\",\n",
      "    \"build_hash\" : \"09df99393193b2c53d92899662a8b8b3c55b45cd\",\n",
      "    \"build_date\" : \"2024-03-22T03:35:46.757803203Z\",\n",
      "    \"build_snapshot\" : false,\n",
      "    \"lucene_version\" : \"9.10.0\",\n",
      "    \"minimum_wire_compatibility_version\" : \"7.17.0\",\n",
      "    \"minimum_index_compatibility_version\" : \"7.0.0\"\n",
      "  },\n",
      "  \"tagline\" : \"You Know, for Search\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "print(requests.get(\"http://localhost:9200\").text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "564e2ac4-92b9-4637-8c35-9e3e9967881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"places_danang\"\n",
    "\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"type\": {\"type\": \"keyword\"},\n",
    "            \"name\": {\"type\": \"text\"},\n",
    "            \"description\": {\"type\": \"text\"},\n",
    "            \"time\": {\"type\": \"keyword\"},\n",
    "            \"price\": {\"type\": \"keyword\"},\n",
    "            \"location\": {\"type\": \"text\"},\n",
    "            \"area\": {\"type\": \"keyword\"},\n",
    "            \"note\": {\"type\": \"text\"},\n",
    "            \"id\": {\"type\": \"keyword\"},\n",
    "            \"full_text\": {\"type\": \"text\"},  \n",
    "            \"vector_search\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,  \n",
    "                \"index\": True,           \n",
    "                \"similarity\": \"cosine\"  \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "217b19e0-1735-449f-b2e2-69aff8d017e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping error details: {'error': {'root_cause': [{'type': 'resource_already_exists_exception', 'reason': 'index [places_danang/SeEZJwL5SbqvNzcifcZgYQ] already exists', 'index_uuid': 'SeEZJwL5SbqvNzcifcZgYQ', 'index': 'places_danang'}], 'type': 'resource_already_exists_exception', 'reason': 'index [places_danang/SeEZJwL5SbqvNzcifcZgYQ] already exists', 'index_uuid': 'SeEZJwL5SbqvNzcifcZgYQ', 'index': 'places_danang'}, 'status': 400}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    es.indices.create(index=index_name, body=mapping)\n",
    "except Exception as e:\n",
    "    print(\"Mapping error details:\", getattr(e, 'info', str(e)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6cc909e-d5b5-4532-9004-60caa6f557fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index `places_danang` đã được tạo!\n"
     ]
    }
   ],
   "source": [
    "#Delete pervious index and create a new one: \n",
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "es.indices.create(index=index_name, body=mapping)\n",
    "print(f\"Index `{index_name}` đã được tạo!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cbdbfff-f66a-45a3-a1f4-53fd813d8d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2216a401-3339-4b29-9ef5-78647ac84c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 299/299 [00:04<00:00, 66.15it/s]\n",
      "100%|████████████████████████████████████████| 299/299 [00:01<00:00, 231.67it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data_danang_ok.csv\")  # Đường dẫn file của bạn\n",
    "\n",
    "# Tạo embedding cho từng record\n",
    "def embed(text):\n",
    "    return model.encode(text).tolist()\n",
    "\n",
    "# Nếu cột vector_search đã có, bỏ qua đoạn này, còn không:\n",
    "tqdm.pandas()\n",
    "df[\"vector_search\"] = df[\"full_text\"].progress_apply(embed)\n",
    "\n",
    "#indexing data to elasticsearch\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    doc = row.to_dict()\n",
    "    # Nếu vector_search dạng numpy, cần chuyển sang list\n",
    "    es.index(index=index_name, id=doc[\"id\"], document=doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc972747-b049-4957-9a77-cf733aea8500",
   "metadata": {},
   "source": [
    "**Querry processing for better search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b718833c-5020-467d-aa9a-bb430e7c6e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom keybert import KeyBERT\\nkw_model = KeyBERT(model=\\'all-MiniLM-L6-v2\\')  # Dùng chính model bạn dùng cho embedding\\n\\ndef extract_keyphrases(query, top_n=3):\\n    keywords = kw_model.extract_keywords(\\n        query, \\n        top_n=top_n, \\n        stop_words=\"english\", \\n        use_maxsum=True,    # Thử các phương pháp diversity\\n        nr_candidates=20,   # Số candidate nhiều hơn\\n        ngram_range=(2, 4)  # Xét cả bigram, trigram, fourgram\\n    )\\n    return [k[0] for k in keywords]\\n\\nprint(extract_keyphrases(\"Where to stay near My Khe Beach\"))\\n# Nhiều khả năng sẽ trả về [\\'my khe beach\\', ...]\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from keybert import KeyBERT\n",
    "kw_model = KeyBERT(model='all-MiniLM-L6-v2')  # Dùng chính model bạn dùng cho embedding\n",
    "\n",
    "def extract_keyphrases(query, top_n=3):\n",
    "    keywords = kw_model.extract_keywords(\n",
    "        query, \n",
    "        top_n=top_n, \n",
    "        stop_words=\"english\", \n",
    "        use_maxsum=True,    # Thử các phương pháp diversity\n",
    "        nr_candidates=20,   # Số candidate nhiều hơn\n",
    "        ngram_range=(2, 4)  # Xét cả bigram, trigram, fourgram\n",
    "    )\n",
    "    return [k[0] for k in keywords]\n",
    "\n",
    "print(extract_keyphrases(\"Where to stay near My Khe Beach\"))\n",
    "# Nhiều khả năng sẽ trả về ['my khe beach', ...]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60ddee44-6a79-414d-96ef-aa939e504b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = {\n",
    "    \"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"for\", \"if\",\n",
    "    \"in\", \"into\", \"is\", \"it\", \"no\", \"not\", \"of\", \"on\", \"or\", \"such\", \"that\",\n",
    "    \"the\", \"their\", \"then\", \"there\", \"these\", \"they\", \"this\", \"to\", \"was\",\n",
    "    \"will\", \"with\", \"me\", \"my\", \"you\", \"your\", \"we\", \"our\", \"us\", \"he\",\n",
    "    \"him\", \"his\", \"she\", \"her\", \"hers\", \"it\", \"its\", \"them\", \"so\", \"too\"\n",
    "}\n",
    "\n",
    "def preprocess_query_for_vector(query):\n",
    "    # Bỏ dấu câu (tuỳ chọn, để nguyên cũng được vì embedding model hiểu)\n",
    "    query_no_punct = re.sub(r'[^\\w\\s]', '', query)\n",
    "    # Bỏ stopword, giữ lại trật tự và ý nghĩa câu\n",
    "    words = query_no_punct.split()\n",
    "    filtered = [w for w in words if w.lower() not in STOP_WORDS]\n",
    "    # Ghép lại thành câu ngắn gọn\n",
    "    processed_query = \" \".join(filtered) if filtered else query\n",
    "    return processed_query\n",
    "\n",
    "# Ví dụ:\n",
    "# \"Where to eat Bun Bo Hue in the evening?\" -> \"Where eat Bun Bo Hue evening\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab401106-18f2-48fa-9ce1-a7a66e73e2e0",
   "metadata": {},
   "source": [
    "**Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "09f4bf9c-9031-4fee-9f49-ff4e746838e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_search(query, top_k=10):\n",
    "    body = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": query,\n",
    "                \"fields\": [\"full_text\"],\n",
    "                #\"operator\": \"or\",\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    res = es.search(index=index_name, body=body)\n",
    "    return [\n",
    "        {\n",
    "            \"id\": hit[\"_source\"][\"id\"],\n",
    "            \"score\": hit[\"_score\"],\n",
    "            \"full_text\": hit[\"_source\"][\"full_text\"]\n",
    "        }\n",
    "        for hit in res[\"hits\"][\"hits\"]\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "15333771-f52d-4e6f-913f-b66cd2bae04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(query, top_k=10):\n",
    "    query_vec = model.encode(query).tolist()\n",
    "    body = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\"match_all\": {}},\n",
    "                \"script\": {\n",
    "                    \"source\": \"cosineSimilarity(params.query_vector, 'vector_search') + 1.0\",\n",
    "                    \"params\": {\"query_vector\": query_vec}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    res = es.search(index=index_name, body=body)\n",
    "    return [\n",
    "        {\n",
    "            \"id\": hit[\"_source\"][\"id\"],\n",
    "            \"score\": hit[\"_score\"],\n",
    "            \"full_text\": hit[\"_source\"][\"full_text\"]\n",
    "        }\n",
    "        for hit in res[\"hits\"][\"hits\"]\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "97e3890e-2865-4dce-9a71-fa547b3c0be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(lexical_hits, semantic_hits, k=60, top_k=5):\n",
    "    rrf_scores = {}\n",
    "    # Process lexical hits\n",
    "    for rank, hit in enumerate(lexical_hits, start=1):\n",
    "        doc_id = hit[\"id\"]\n",
    "        score = 1 / (k + rank)\n",
    "        if doc_id in rrf_scores:\n",
    "            rrf_scores[doc_id][\"rrf_score\"] += score\n",
    "            rrf_scores[doc_id][\"lexical_score\"] = hit[\"score\"]\n",
    "        else:\n",
    "            rrf_scores[doc_id] = {\n",
    "                \"full_text\": hit.get(\"full_text\", \"\"),\n",
    "                \"lexical_score\": hit[\"score\"],\n",
    "                \"semantic_score\": 0,\n",
    "                \"rrf_score\": score\n",
    "            }\n",
    "    # Process semantic hits\n",
    "    for rank, hit in enumerate(semantic_hits, start=1):\n",
    "        doc_id = hit[\"id\"]\n",
    "        score = 1 / (k + rank)\n",
    "        if doc_id in rrf_scores:\n",
    "            rrf_scores[doc_id][\"rrf_score\"] += score\n",
    "            rrf_scores[doc_id][\"semantic_score\"] = hit[\"score\"]\n",
    "        else:\n",
    "            rrf_scores[doc_id] = {\n",
    "                \"full_text\": hit.get(\"full_text\", \"\"),\n",
    "                \"lexical_score\": 0,\n",
    "                \"semantic_score\": hit[\"score\"],\n",
    "                \"rrf_score\": score\n",
    "            }\n",
    "    # Sort by rrf_score\n",
    "    results = sorted(rrf_scores.values(), key=lambda x: x[\"rrf_score\"], reverse=True)[:top_k]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "43bf3b8a-8cd2-4eca-84f4-4982e36283cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def hybrid_search(query, top_k=5, alpha=0.5):\n",
    "    # BM25\n",
    "    bm25_results = bm25_search(query, top_k=top_k*2)\n",
    "    bm25_ids = {doc[\"id\"]: doc for doc in bm25_results}\n",
    "\n",
    "    # Vector\n",
    "    vec_results = vector_search(query, top_k=top_k*2)\n",
    "    vec_ids = {doc[\"id\"]: doc for doc in vec_results}\n",
    "\n",
    "    # Gộp tất cả id\n",
    "    all_ids = set(bm25_ids.keys()) | set(vec_ids.keys())\n",
    "\n",
    "    # Tính điểm hybrid\n",
    "    hybrid_results = []\n",
    "    for id_ in all_ids:\n",
    "        bm25_score = bm25_ids.get(id_, {}).get(\"score\", 0)\n",
    "        vec_score = vec_ids.get(id_, {}).get(\"score\", 0)\n",
    "        score = (1 - alpha) * bm25_score + alpha * vec_score\n",
    "        hybrid_results.append({\n",
    "            \"id\": id_,\n",
    "            \"hybrid_score\": score,\n",
    "            \"full_text\": bm25_ids.get(id_, vec_ids.get(id_, {})).get(\"full_text\", \"\")\n",
    "        })\n",
    "\n",
    "    # Sort theo điểm hybrid\n",
    "    hybrid_results = sorted(hybrid_results, key=lambda x: x[\"hybrid_score\"], reverse=True)[:top_k]\n",
    "    return hybrid_results\n",
    "'''\n",
    "def hybrid_search(query, top_k=5, k_rrf=60):\n",
    "    bm25_results = bm25_search(query, top_k=top_k*2)   # Lấy nhiều hơn để RRF hiệu quả hơn\n",
    "    vector_results = vector_search(query, top_k=top_k*2)\n",
    "    results = reciprocal_rank_fusion(bm25_results, vector_results, k=k_rrf, top_k=top_k)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e058d148-5698-47b2-8bfe-fcf21e918f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid: hue-style spicy beef noodle soup morning 35k VND (~1.4$) 23 Ngo Gia Tu center Bun Bo Ba Thuong offers spicy broth with lemongrass, beef shank, and thick round noodles.\n"
     ]
    }
   ],
   "source": [
    "query = \"Where to eat Bun Bo Hue in the evening\"\n",
    "#print(\"BM25:\", bm25_search(query)[1]['full_text'])\n",
    "#print(\"Vector:\", vector_search(query)[0]['full_text'])\n",
    "print(\"Hybrid:\", hybrid_search(query)[0]['full_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ee5d424-6b9b-4ce0-a405-77879ad9e842",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def preprocess_query_for_bm25(query, min_word_len=2):\n",
    "    # Làm sạch: bỏ dấu câu, lowercase\n",
    "    cleaned = re.sub(r'[^\\w\\s]', '', query).lower()\n",
    "    # Loại stopword, từ ngắn\n",
    "    words = cleaned.split()\n",
    "    filtered_words = [word for word in words if word not in STOP_WORDS and len(word) >= min_word_len]\n",
    "    cleaned_query = \" \".join(filtered_words) if filtered_words else cleaned\n",
    "    # Trích xuất noun phrase từ cleaned_query\n",
    "    doc = nlp(cleaned_query)\n",
    "    noun_phrases = [chunk.text.strip() for chunk in doc.noun_chunks if chunk.text.strip()]\n",
    "    # Nếu có noun phrase thì trả về list đó, còn không thì trả về cleaned_query\n",
    "    return noun_phrases if noun_phrases else [cleaned_query]\n",
    "\n",
    "# Ví dụ:\n",
    "# \"Where to eat Bun Bo Hue in the evening?\" -> ['bun bo hue', 'evening']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b72ec4f-2ff0-4dc0-8e11-71ca4836abec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c175ef-6868-442d-beda-f067984a5160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a7aeb1d-eead-4875-817c-6eba7ac2a0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04b294a-6264-4126-b96f-b5f042e19982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
