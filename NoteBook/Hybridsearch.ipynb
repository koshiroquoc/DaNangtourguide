{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7796e974-8562-4d81-bef0-cd63f19228d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /usr/local/python/3.12.1/lib/python3.12/site-packages (4.1.0)\n",
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from sentence-transformers) (4.52.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (2.6.0+cpu)\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from sentence-transformers) (0.32.4)\n",
      "Requirement already satisfied: Pillow in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/python/3.12.1/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.1.31)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ede5f54e-0bee-496b-b9a2-c2ac9051c1ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n",
      "Requirement already satisfied: keybert in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/codespace/.local/lib/python3.12/site-packages (from keybert) (2.2.4)\n",
      "Requirement already satisfied: rich>=10.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from keybert) (14.0.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in /home/codespace/.local/lib/python3.12/site-packages (from keybert) (1.6.1)\n",
      "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from keybert) (4.1.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich>=10.4.0->keybert) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn>=0.22.2->keybert) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn>=0.22.2->keybert) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn>=0.22.2->keybert) (3.6.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from sentence-transformers>=0.3.8->keybert) (4.52.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from sentence-transformers>=0.3.8->keybert) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers>=0.3.8->keybert) (2.6.0+cpu)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from sentence-transformers>=0.3.8->keybert) (0.32.4)\n",
      "Requirement already satisfied: Pillow in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers>=0.3.8->keybert) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers>=0.3.8->keybert) (4.12.2)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (3.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2024.6.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (1.1.3)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/python/3.12.1/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md\n",
    "!pip install keybert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4416b89-7596-4f78-a831-39e6f232df77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: elasticsearch<9 in /usr/local/python/3.12.1/lib/python3.12/site-packages (8.18.1)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.15.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from elasticsearch<9) (8.17.1)\n",
      "Requirement already satisfied: python-dateutil in /home/codespace/.local/lib/python3.12/site-packages (from elasticsearch<9) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions in /home/codespace/.local/lib/python3.12/site-packages (from elasticsearch<9) (4.12.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in /home/codespace/.local/lib/python3.12/site-packages (from elastic-transport<9,>=8.15.1->elasticsearch<9) (2.3.0)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from elastic-transport<9,>=8.15.1->elasticsearch<9) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil->elasticsearch<9) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"elasticsearch<9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbfb9fb3-560b-4ae8-99e5-c895b6b6db37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0845664-56ae-4741-a0c7-e901b75e80e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\n",
    "    hosts=[\"http://localhost:9200\"],   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7966753-dbdd-41c6-b83a-901365080ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\" : \"941bc3af368d\",\n",
      "  \"cluster_name\" : \"docker-cluster\",\n",
      "  \"cluster_uuid\" : \"6KQrMvN8QuyudDcEoPEIBA\",\n",
      "  \"version\" : {\n",
      "    \"number\" : \"8.13.0\",\n",
      "    \"build_flavor\" : \"default\",\n",
      "    \"build_type\" : \"docker\",\n",
      "    \"build_hash\" : \"09df99393193b2c53d92899662a8b8b3c55b45cd\",\n",
      "    \"build_date\" : \"2024-03-22T03:35:46.757803203Z\",\n",
      "    \"build_snapshot\" : false,\n",
      "    \"lucene_version\" : \"9.10.0\",\n",
      "    \"minimum_wire_compatibility_version\" : \"7.17.0\",\n",
      "    \"minimum_index_compatibility_version\" : \"7.0.0\"\n",
      "  },\n",
      "  \"tagline\" : \"You Know, for Search\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "print(requests.get(\"http://localhost:9200\").text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "564e2ac4-92b9-4637-8c35-9e3e9967881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"places_danang\"\n",
    "\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"type\": {\"type\": \"keyword\"},\n",
    "            \"name\": {\"type\": \"text\"},\n",
    "            \"description\": {\"type\": \"text\"},\n",
    "            \"time\": {\"type\": \"keyword\"},\n",
    "            \"price\": {\"type\": \"keyword\"},\n",
    "            \"location\": {\"type\": \"text\"},\n",
    "            \"area\": {\"type\": \"keyword\"},\n",
    "            \"note\": {\"type\": \"text\"},\n",
    "            \"id\": {\"type\": \"keyword\"},\n",
    "            \"full_text\": {\"type\": \"text\"},  \n",
    "            \"vector_search\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,  \n",
    "                \"index\": True,           \n",
    "                \"similarity\": \"cosine\"  \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "217b19e0-1735-449f-b2e2-69aff8d017e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping error details: {'error': {'root_cause': [{'type': 'resource_already_exists_exception', 'reason': 'index [places_danang/ultuLYPyRlejd7t699_KKw] already exists', 'index_uuid': 'ultuLYPyRlejd7t699_KKw', 'index': 'places_danang'}], 'type': 'resource_already_exists_exception', 'reason': 'index [places_danang/ultuLYPyRlejd7t699_KKw] already exists', 'index_uuid': 'ultuLYPyRlejd7t699_KKw', 'index': 'places_danang'}, 'status': 400}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    es.indices.create(index=index_name, body=mapping)\n",
    "except Exception as e:\n",
    "    print(\"Mapping error details:\", getattr(e, 'info', str(e)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6cc909e-d5b5-4532-9004-60caa6f557fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index `places_danang` đã được tạo!\n"
     ]
    }
   ],
   "source": [
    "#Delete pervious index and create a new one: \n",
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "es.indices.create(index=index_name, body=mapping)\n",
    "print(f\"Index `{index_name}` đã được tạo!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cbdbfff-f66a-45a3-a1f4-53fd813d8d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2216a401-3339-4b29-9ef5-78647ac84c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 299/299 [00:05<00:00, 58.76it/s]\n",
      "100%|███████████████████████████████| 299/299 [00:02<00:00, 128.82it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data_danang_ok.csv\")  # Đường dẫn file của bạn\n",
    "\n",
    "# Tạo embedding cho từng record\n",
    "def embed(text):\n",
    "    return model.encode(text).tolist()\n",
    "\n",
    "# Nếu cột vector_search đã có, bỏ qua đoạn này, còn không:\n",
    "tqdm.pandas()\n",
    "df[\"vector_search\"] = df[\"full_text\"].progress_apply(embed)\n",
    "\n",
    "#indexing data to elasticsearch\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    doc = row.to_dict()\n",
    "    # Nếu vector_search dạng numpy, cần chuyển sang list\n",
    "    es.index(index=index_name, id=doc[\"id\"], document=doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc972747-b049-4957-9a77-cf733aea8500",
   "metadata": {},
   "source": [
    "**Querry processing for better search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b718833c-5020-467d-aa9a-bb430e7c6e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a noodle soup breakfast center'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract keywords for better keyword search\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "def preprocess_bm25_query(query):\n",
    "    doc = nlp(query)\n",
    "    return \" \".join([chunk.text.strip() for chunk in doc.noun_chunks if len(chunk.text.strip()) > 2])\n",
    "#preprocess_bm25_query(\"suggest a noodle soup for breakfast near center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60ddee44-6a79-414d-96ef-aa939e504b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Where eat Bun Bo Hue evening'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove non-sense words \n",
    "STOP_WORDS = {\n",
    "    \"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"for\", \"if\",\n",
    "    \"in\", \"into\", \"is\", \"it\", \"no\", \"not\", \"of\", \"on\", \"or\", \"such\", \"that\",\n",
    "    \"the\", \"their\", \"then\", \"there\", \"these\", \"they\", \"this\", \"to\", \"was\",\n",
    "    \"will\", \"with\", \"me\", \"my\", \"you\", \"your\", \"we\", \"our\", \"us\", \"he\",\n",
    "    \"him\", \"his\", \"she\", \"her\", \"hers\", \"it\", \"its\", \"them\", \"so\", \"too\"\n",
    "}\n",
    "\n",
    "def preprocess_query_for_vector(query):\n",
    "    # Bỏ dấu câu (tuỳ chọn, để nguyên cũng được vì embedding model hiểu)\n",
    "    query_no_punct = re.sub(r'[^\\w\\s]', '', query)\n",
    "    # Bỏ stopword, giữ lại trật tự và ý nghĩa câu\n",
    "    words = query_no_punct.split()\n",
    "    filtered = [w for w in words if w.lower() not in STOP_WORDS]\n",
    "    # Ghép lại thành câu ngắn gọn\n",
    "    processed_query = \" \".join(filtered) if filtered else query\n",
    "    return processed_query\n",
    "\n",
    "# Ví dụ:\n",
    "#preprocess_query_for_vector( \"Where to eat Bun Bo Hue in the evening?\")\n",
    "    #-> \"Where eat Bun Bo Hue evening\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab401106-18f2-48fa-9ce1-a7a66e73e2e0",
   "metadata": {},
   "source": [
    "**Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "09f4bf9c-9031-4fee-9f49-ff4e746838e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def bm25_search(query, top_k=10):\n",
    "    processed_query = preprocess_bm25_query(query)  # extract keyword\n",
    "    body = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": processed_query,\n",
    "                \"fields\": [\"name^3\", \"description^2\", \"note\", \"full_text\"],\n",
    "                \"operator\": \"or\",  \n",
    "                \"type\": \"most_fields\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    res = es.search(index=index_name, body=body)\n",
    "    return [\n",
    "        {\n",
    "            \"id\": hit[\"_source\"][\"id\"],\n",
    "            \"score\": hit[\"_score\"],\n",
    "            \"full_text\": hit[\"_source\"][\"full_text\"]\n",
    "        }\n",
    "        for hit in res[\"hits\"][\"hits\"]\n",
    "    ]\n",
    "'''\n",
    "def bm25_search(query, top_k=10, type_filter=None):\n",
    "    processed_query = preprocess_bm25_query(query)\n",
    "\n",
    "    must_clauses = [\n",
    "        {\n",
    "            \"multi_match\": {\n",
    "                \"query\": processed_query,\n",
    "                \"fields\": [\"name^3\", \"description^2\", \"note\", \"full_text\"],\n",
    "                \"operator\": \"or\",\n",
    "                \"type\": \"most_fields\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Nếu có filter, thêm điều kiện\n",
    "    if type_filter:\n",
    "        must_clauses.append({\"term\": {\"type\": type_filter}})\n",
    "\n",
    "    body = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": must_clauses\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    res = es.search(index=index_name, body=body)\n",
    "    return [\n",
    "        {\n",
    "            \"id\": hit[\"_source\"][\"id\"],\n",
    "            \"score\": hit[\"_score\"],\n",
    "            \"name\": hit[\"_source\"][\"name\"],\n",
    "            \"description\": hit[\"_source\"][\"description\"],\n",
    "            \"time\": hit[\"_source\"][\"time\"],\n",
    "            \"price\": hit[\"_source\"][\"price\"],\n",
    "            \"location\": hit[\"_source\"][\"location\"],\n",
    "            \"area\": hit[\"_source\"][\"area\"],\n",
    "            \"note\": hit[\"_source\"][\"note\"],\n",
    "            \"type\": hit[\"_source\"][\"type\"]\n",
    "        }\n",
    "        for hit in res[\"hits\"][\"hits\"]\n",
    "    ]\n",
    "#khi goi bm25_search(query, type_filter=\"eat\") -> tim trong moi muc eat thoi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "15333771-f52d-4e6f-913f-b66cd2bae04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def vector_search(query, top_k=10):\n",
    "    query_vec = model.encode(query).tolist()\n",
    "    body = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\"match_all\": {}},\n",
    "                \"script\": {\n",
    "                    \"source\": \"cosineSimilarity(params.query_vector, 'vector_search') + 1.0\",\n",
    "                    \"params\": {\"query_vector\": query_vec}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    res = es.search(index=index_name, body=body)\n",
    "    return [\n",
    "        {\n",
    "            \"id\": hit[\"_source\"][\"id\"],\n",
    "            \"score\": hit[\"_score\"],\n",
    "            \"full_text\": hit[\"_source\"][\"full_text\"]\n",
    "        }\n",
    "        for hit in res[\"hits\"][\"hits\"]\n",
    "    ]\n",
    "'''\n",
    "def vector_search(query, top_k=10, type_filter=None):\n",
    "    query = preprocess_query_for_vector(query)\n",
    "    query_vec = model.encode(query).tolist()\n",
    "\n",
    "    # Nếu có filter, dùng bool; nếu không, dùng match_all như cũ\n",
    "    if type_filter:\n",
    "        inner_query = {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\"term\": {\"type\": type_filter}}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        inner_query = {\"match_all\": {}}\n",
    "\n",
    "    body = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": inner_query,\n",
    "                \"script\": {\n",
    "                    \"source\": \"cosineSimilarity(params.query_vector, 'vector_search') + 1.0\",\n",
    "                    \"params\": {\"query_vector\": query_vec}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    res = es.search(index=index_name, body=body)\n",
    "    return [\n",
    "        {\n",
    "            \"id\": hit[\"_source\"][\"id\"],\n",
    "            \"score\": hit[\"_score\"],\n",
    "            \"name\": hit[\"_source\"][\"name\"],\n",
    "            \"description\": hit[\"_source\"][\"description\"],\n",
    "            \"time\": hit[\"_source\"][\"time\"],\n",
    "            \"price\": hit[\"_source\"][\"price\"],\n",
    "            \"location\": hit[\"_source\"][\"location\"],\n",
    "            \"area\": hit[\"_source\"][\"area\"],\n",
    "            \"note\": hit[\"_source\"][\"note\"],\n",
    "            \"type\": hit[\"_source\"][\"type\"]\n",
    "        }\n",
    "        for hit in res[\"hits\"][\"hits\"]\n",
    "    ]\n",
    "#vector_search(query, type_filter=\"eat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "97e3890e-2865-4dce-9a71-fa547b3c0be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(lexical_hits, semantic_hits, k=60, top_k=5):\n",
    "    rrf_scores = {}\n",
    "    # Lexical hits\n",
    "    for rank, hit in enumerate(lexical_hits, start=1):\n",
    "        doc_id = hit[\"id\"]\n",
    "        score = 1 / (k + rank)\n",
    "        if doc_id in rrf_scores:\n",
    "            rrf_scores[doc_id][\"rrf_score\"] += score\n",
    "            rrf_scores[doc_id][\"lexical_score\"] = hit[\"score\"]\n",
    "        else:\n",
    "            rrf_scores[doc_id] = {**hit, \"lexical_score\": hit[\"score\"], \"semantic_score\": 0, \"rrf_score\": score}\n",
    "    # Semantic hits\n",
    "    for rank, hit in enumerate(semantic_hits, start=1):\n",
    "        doc_id = hit[\"id\"]\n",
    "        score = 1 / (k + rank)\n",
    "        if doc_id in rrf_scores:\n",
    "            rrf_scores[doc_id][\"rrf_score\"] += score\n",
    "            rrf_scores[doc_id][\"semantic_score\"] = hit[\"score\"]\n",
    "        else:\n",
    "            rrf_scores[doc_id] = {**hit, \"lexical_score\": 0, \"semantic_score\": hit[\"score\"], \"rrf_score\": score}\n",
    "    results = sorted(rrf_scores.values(), key=lambda x: x[\"rrf_score\"], reverse=True)[:top_k]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "43bf3b8a-8cd2-4eca-84f4-4982e36283cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def hybrid_search(query, top_k=5, alpha=0.5):\n",
    "    # BM25\n",
    "    bm25_results = bm25_search(query, top_k=top_k*2)\n",
    "    bm25_ids = {doc[\"id\"]: doc for doc in bm25_results}\n",
    "\n",
    "    # Vector\n",
    "    vec_results = vector_search(query, top_k=top_k*2)\n",
    "    vec_ids = {doc[\"id\"]: doc for doc in vec_results}\n",
    "\n",
    "    # Gộp tất cả id\n",
    "    all_ids = set(bm25_ids.keys()) | set(vec_ids.keys())\n",
    "\n",
    "    # Tính điểm hybrid\n",
    "    hybrid_results = []\n",
    "    for id_ in all_ids:\n",
    "        bm25_score = bm25_ids.get(id_, {}).get(\"score\", 0)\n",
    "        vec_score = vec_ids.get(id_, {}).get(\"score\", 0)\n",
    "        score = (1 - alpha) * bm25_score + alpha * vec_score\n",
    "        hybrid_results.append({\n",
    "            \"id\": id_,\n",
    "            \"hybrid_score\": score,\n",
    "            \"full_text\": bm25_ids.get(id_, vec_ids.get(id_, {})).get(\"full_text\", \"\")\n",
    "        })\n",
    "\n",
    "    # Sort theo điểm hybrid\n",
    "    hybrid_results = sorted(hybrid_results, key=lambda x: x[\"hybrid_score\"], reverse=True)[:top_k]\n",
    "    return hybrid_results\n",
    "'''\n",
    "def hybrid_search(query, top_k=5, k_rrf=60, type_filter=None):\n",
    "    bm25_results = bm25_search(query, top_k=top_k*2,type_filter=type_filter )   # Lấy nhiều hơn để RRF hiệu quả hơn\n",
    "    vector_results = vector_search(query, top_k=top_k*2, type_filter=type_filter)\n",
    "    results = reciprocal_rank_fusion(bm25_results, vector_results, k=k_rrf, top_k=top_k)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e1f3a3-de70-4bba-b270-3cbbb73ad64d",
   "metadata": {},
   "source": [
    "**Testing search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e058d148-5698-47b2-8bfe-fcf21e918f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query = \"Where to eat beef noodles\"\n",
    "#print(\"BM25:\", bm25_search(query, type_filter = 'eat'))\n",
    "#print(\"Vector:\", vector_search(query, type_filter=\"see\"))\n",
    "#print(\"Hybrid:\", hybrid_search(query, type_filter=\"eat\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc94ad2-4db9-449e-b91c-587ed8cc876d",
   "metadata": {},
   "source": [
    "**Build Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8ee5d424-6b9b-4ce0-a405-77879ad9e842",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_template = \"\"\"\n",
    "Name: {name}\n",
    "Type: {type}\n",
    "Description: {description}\n",
    "Time: {time}\n",
    "Price: {price}\n",
    "Location: {location}\n",
    "Area: {area}\n",
    "Note: {note}\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are a helpful local travel assistant for Da Nang. Answer the QUESTION based on the CONTEXT from our database of places to eat, see, and stay.\n",
    "Only use the facts from the CONTEXT when answering the QUESTION. If you don't know, say you don't know.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4b72ec4f-2ff0-4dc0-8e11-71ca4836abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    context = \"\"\n",
    "    for doc in search_results:\n",
    "        context += entry_template.format(**doc) + \"\\n\\n\"\n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c0c175ef-6868-442d-beda-f067984a5160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful local travel assistant for Da Nang. Answer the QUESTION based on the CONTEXT from our database of places to eat, see, and stay.\n",
      "Only use the facts from the CONTEXT when answering the QUESTION. If you don't know, say you don't know.\n",
      "\n",
      "QUESTION: Where can I eat grilled fish in Da Nang?\n",
      "\n",
      "CONTEXT:\n",
      "Name: Bun Cha Ca Ba Lu\n",
      "Type: eat\n",
      "Description: grilled fish noodle soup\n",
      "Time: morning\n",
      "Price: 35k VND (~1.4$)\n",
      "Location: 319 Hung Vuong\n",
      "Area: center\n",
      "Note: Bun Cha Ca Ba Lu is known for its rich fish-based broth, crispy fried fish cakes, and fresh herbs.\n",
      "\n",
      "Name: Bun Cha Ca Ong Ta\n",
      "Type: eat\n",
      "Description: grilled fish cake noodle soup\n",
      "Time: morning\n",
      "Price: 35k VND (~1.4$)\n",
      "Location: 113 Nguyen Van Thoai\n",
      "Area: beach\n",
      "Note: Ong Ta’s Bun Cha Ca is known for tender fish cake slices in a mildly sweet and spicy broth with green onions.\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"Where can I eat grilled fish in Da Nang?\"\n",
    "search_results = hybrid_search(sample_query, top_k=2, type_filter=\"eat\")  # hoặc merged từ RRF\n",
    "prompt = build_prompt(sample_query, search_results)\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8a7aeb1d-eead-4875-817c-6eba7ac2a0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama',  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c04b294a-6264-4126-b96f-b5f042e19982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model='llama3.1',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "095583df-d71d-4d28-a105-e0d10fad08d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Vietnam is Hanoi.\n"
     ]
    }
   ],
   "source": [
    "#test llm\n",
    "#print(llm(\"Hello, what is the capital of Viet Nam?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3684ee58-8998-4db2-81ff-c1522c4cf618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query, type_filter=None, top_k=3):\n",
    "    search_results = hybrid_search(query, top_k=top_k, type_filter=type_filter)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer\n",
    "\n",
    "# Test end-to-end\n",
    "#print(rag(\"Suggest a noodle soup for breakfast in the center\", model=\"llama3.1\", type_filter=\"eat\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
